{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9005c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch transformers accelerate bitsandbytes langchain sentence_transformers faiss-gpu openpyxl pacmap datasets langchain-community ragatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1821c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", None\n",
    ")  # This will be helpful when visualizing retriever outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a86f71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict\n",
    "import os\n",
    "\n",
    "def extract_pdf_with_langchain(pdf_path: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract and process text from a PDF using LangChain.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of text chunks for splitting\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing raw text, chunked text, and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the PDF loader\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        \n",
    "        # Load the document\n",
    "        pages = loader.load()\n",
    "        \n",
    "        # Extract raw text and metadata\n",
    "        raw_text = '\\n'.join([page.page_content for page in pages])\n",
    "        metadata = [page.metadata for page in pages]\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "        \n",
    "        # Split text into chunks\n",
    "        chunks = text_splitter.split_text(raw_text)\n",
    "        \n",
    "        return {\n",
    "            'raw_text': raw_text,\n",
    "            'chunks': chunks,\n",
    "            'metadata': metadata,\n",
    "            'num_pages': len(pages),\n",
    "            'num_chunks': len(chunks)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def batch_process_pdfs(directory_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process multiple PDFs in a directory.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to directory containing PDFs\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing processed PDF data\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            pdf_path = os.path.join(directory_path, filename)\n",
    "            result = extract_pdf_with_langchain(pdf_path)\n",
    "            if result:\n",
    "                result['filename'] = filename\n",
    "                results.append(result)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bb42722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Meta-2021-Sustainability-Report.pdf  'RAG (1).ipynb'\t       Untitled.ipynb\r\n",
      "'Pymongo (1).ipynb'\t\t       RAG_GENAI.ipynb\r\n",
      " PyMongo_rag.ipynb\t\t       RAG_HuggingFace.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e3611e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(number of pdfs you have)\n",
    "pdf_path = \"Meta-2021-Sustainability-Report.pdf\"\n",
    "result = extract_pdf_with_langchain(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4360ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Processed PDF with {result['num_pages']} pages\")\n",
    "# print(f\"Generated {result['num_chunks']} chunks\")\n",
    "# print(\"\\nFirst chunk of text:\")\n",
    "# if result['chunks']:\n",
    "#     print(result['chunks'][0])\n",
    "\n",
    "# print(\"\\nMetadata for first page:\")\n",
    "# if result['metadata']:\n",
    "#     print(result['metadata'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce48f50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'source'],\n",
       "    num_rows: 2647\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be7b0008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e5310a79cd4c49b7de77534c2df1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "061996d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "# This list is taken from LangChain's MarkdownTextSplitter class\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n",
    "    chunk_overlap=100,  # The number of characters to overlap between chunks\n",
    "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
    "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
    "    separators=MARKDOWN_SEPARATORS,\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in RAW_KNOWLEDGE_BASE:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "feb04132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31085"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a692858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13e069de",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = 'thenlper/gte-small'\n",
    "\n",
    "def split_documents(chunk_size: int,\n",
    "                knowledge_base: List[LangchainDocument],\n",
    "                tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME)-> List[LangchainDocument]:\n",
    "\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "          AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "          chunk_size=chunk_size,\n",
    "          chunk_overlap=100,\n",
    "          add_start_index= True,\n",
    "          strip_whitespace= True,\n",
    "          separators = MARKDOWN_SEPARATORS)\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    return docs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7c62032",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_processed = split_documents(512,\n",
    "                                 RAW_KNOWLEDGE_BASE,\n",
    "                                 tokenizer_name = EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8db675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")\n",
    "\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "247d1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \" How to create a transformer pipeline object?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34b361ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = embedding_model.embed_query(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea6dbfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_documents = KNOWLEDGE_VECTOR_DATABASE.similarity_search(user_query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40b15e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(related_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec851874",
   "metadata": {},
   "source": [
    "# Second Part of RAG : Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc62791",
   "metadata": {},
   "source": [
    "OpenAI api - $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ffb271b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e41bfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2fb036af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llm(sp, prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sp},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a794c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=\"\"\"\n",
    "You are acting as a RAG Assistant. \n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "acf39b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(related_document, user_query):\n",
    "    return f\"\"\"<|CONTEXT START|>\n",
    "\n",
    "                                {related_document}\n",
    "                                <|CONTEXT ENDS|>\n",
    "                                Now  here is the question you need to answer\n",
    "                                {user_query}\n",
    "                                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dfffa971",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_docs = ''\n",
    "for r in related_documents:\n",
    "    whole_docs += str(r.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4589b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d61864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =get_prompt(whole_docs, user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65ae92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae7769ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To create a Transformer pipeline object, you would typically follow these steps:\\n\\n1. Import the necessary classes from the `transformers` package:\\n   ```python\\n   from transformers import AutoTokenizer, AutoModelForCausalLM\\n   ```\\n\\n2. Load the tokenizer and model using the appropriate names or paths:\\n   ```python\\n   tokenizer = AutoTokenizer.from_pretrained(\"model_name_or_path\", subfolder=\"tokenizer\")\\n   model = AutoModelForCausalLM.from_pretrained(\"model_name_or_path\", subfolder=\"model\")\\n   ```\\n\\nThe exact details will depend on the specific models and tokenizers available in the Hugging Face repository.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = llm(sp , prompt)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "16f726b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(user_query, num_of_docs):\n",
    "    related_documents = KNOWLEDGE_VECTOR_DATABASE.similarity_search(user_query, k=num_of_docs)\n",
    "    whole_docs = ''\n",
    "    for r in related_documents:\n",
    "        whole_docs += str(r.page_content)\n",
    "    prompt =get_prompt(whole_docs, user_query)\n",
    "    res = llm(sp , prompt)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e858a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Transformer-based Sequence-to-Sequence model uses a special type of neural network called a Transformer to process sequences of data. It consists of two main components: an Encoder and a Decoder. The Encoder takes input sequences and encodes them into fixed-length representations, while the Decoder generates output sequences based on these encoded representations. This architecture allows the model to handle long-term dependencies and contextual information effectively, making it suitable for various NLP tasks such as machine translation, text summarization, and question answering.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"What is a Transformer Seq-to-Seq\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3486c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_1",
   "language": "python",
   "name": "genai_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
